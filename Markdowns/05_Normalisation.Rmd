---
title: "Introduction to single-cell RNA-seq analysis"
subtitle: Normalisation
output:
  html_document:
    toc: yes
    number_sections: true
    code_folding: show 
---

# Normalisation

```{r setup, echo=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(error=FALSE, message=FALSE, warning=FALSE, cache=TRUE)
set.seed(123)
```

Acknowledgments: much of the material in this section hase been derived from the 
chapters on normalisation in the
[OSCA book](http://bioconductor.org/books/3.14/OSCA.basic/normalization.html) 
and the [Hemberg Group course materials](https://www.singlecellcourse.org/).

## Learning objectives

* Understand why normalisation is required
* Understand concepts of two normalisation methods
  * deconvolution
  * sctransform 

## Why normalise?

Systematic differences in sequencing coverage between libraries occur because of
low input material, differences in cDNA capture and PCR amplification.
Normalisation removes such differences so that differences between cells are not
technical but biological, allowing meaningful comparison of expression profiles
between cells. Normalisation and batch correction have different aims.
Normalisation addresses technical differences only, while batch correction
considers both technical and biological differences.

```{r load_packages, message=FALSE}
library(scater)
library(scran)
library(sctransform)
library(tidyverse)
library(BiocParallel)

bpp <- MulticoreParam(7)
```

## Load data object

For the purposes of this demonstration we have generated a smaller data set in
which there are only 500 cells per sample. This is so that the code can be run
in a reasonable amount of time during the live teaching session. The data were
first QC'd and filtered as described in the [QC and exploratory analysis 
session](04_Preprocessing_And_QC.html). After this 500 cells were selected at
random from each sample.

```{r load_data}
sce <- readRDS("R_objects/Caron_filtered.500.rds")
sce
table(sce$SampleName)
```

# Scaling normalizations

In scaling normalization, the “normalization factor” is an estimate of the
library size relative to the other cells. Steps usually include: computation of
a cell-specific 'scaling' or 'size' factor that represents the relative bias in
that cell and division of all counts for the cell by that factor to remove that
bias. Assumption: any cell specific bias will affect genes the same way.

Scaling methods typically generate normalised counts-per-million (CPM) or
transcripts-per-million (TPM) values that address the effect of sequencing
depth. These values however typically have a variance that increases with their
mean (heteroscedasticity) while most statistical methods assume a stable
variance, which does not vary with the mean (homoscedasticity). A widely used
'variance stabilising transformation' is the log transformation (often log2).
This works well for highly expressed genes (as in bulk RNA-seq) but less so for
sparse scRNA-seq data.

## CPM

Convert raw counts to counts-per-million (CPM) for each cell by dividing counts
by the library size then multiplying by 1 million. However, this does not
address composition bias caused by highly expressed genes that are also
differentially expressed between cells. In `scuttle` CPMs are computed as
follows:

```
norm_factor <- colSums(expr_mat[-spikes, ])
return(t(t(expr_mat)/norm_factor)) * 10^6
```

## DESeq's size factor

For each gene, compute geometric mean across cells. for each cell compute for
each gene the ratio of its expression to its geometric mean, and derive the
cell's size factor as the median ratio across genes. Not suitable for sparse
scRNA-seq data as the geometric is computed on non-zero values only. This method
is also known as 'Relative Log Expression' (RLE) in `edgeR` and `scater`.

Example code:

```
geomeans <- exp(rowMeans(log(expr_mat[-spikes, ])))
SF <- function(cnts) {
    median((cnts/geomeans)[(is.finite(geomeans) &
		geomeans > 0)])
}
norm_factor <- apply(expr_mat[-spikes, ], 2, SF)
return(t(t(expr_mat)/norm_factor))
```

## Weighted Trimmed mean of M-values

To compute weighted Trimmed mean of M-values (TMM), a given cell is chosen as a
reference to use in computation for other cells. The M-values are gene-wise
log2-fold changes between cells. Trimming entails the removal of the top and
bottom 30% of values. The size factor is computed as the average for the
remaining cells with a weight according to inverse variances. This method
assumes that most genes are not differentially expressed, and the 40% of genes
left after trimming may include many zero counts.

```{r calcNormFactors}
sizeFactors(sce) <- edgeR::calcNormFactors(counts(sce), method = "TMM")
```

## Library size normalization

For each cell, the library size factor is proportional to the library size such
that the average size factor across cell is one.

Advantage: normalised counts are on the same scale as the initial counts.

Compute size factors:

```{r librarySizeFactors}
lib.sf <- librarySizeFactors(sce)
summary(lib.sf)
```

Size factor distribution: wide range, typical of scRNA-seq data.

```{r librarySizeFactors_hist}
data.frame("log10libSf"=log10(lib.sf)) %>% 
  ggplot(aes(x=log10libSf)) + 
    geom_histogram(bins=50)
```

Assumption: absence of composition bias; differential expression between two 
cells is balanced: upregulation in some genes is accompanied by downregulation 
of other genes. Not observed.

Inaccurate normalisation due to unaccounted-for composition bias affects the 
size of the log fold change measured between clusters, but less so the
clustering itself. It is thus sufficient to identify clusters and top marker 
genes.

## Deconvolution

Composition bias occurs when differential expression between two samples or here
cells is not balanced. For a fixed library size, identical in both cells,
upregulation of one gene in a cell will means fewer UMIs can be assigned to
other genes, which would then appear down regulated. Even if library sizes are
allowed to differ, with that for the cell with upregulation being higher,
scaling normalisation will reduce normalised counts. Non-upregulated would
therefore also appear downregulated.

For bulk RNA-seq, composition bias is removed by assuming that most genes are
not differentially expressed between samples, so that differences in non-DE 
genes would amount to the bias, and used to compute size factors.

Given the sparsity of scRNA-seq data, the assumptions are not appropriate.

The deconvolution method increases read counts by pooling cells into groups,
computing size factors within each of these groups. This process is repeated
many times, changing pools each time to collect several size factors for each
cell, from which is derived a single value for that cell.

![](Images/scran_Fig3.png){width=70%}

In order to avoid pooling cells with radically different transcriptomic
profiles, the cells are first clustered based on gene expression. The pools are
then formed exclusively with each cluster. Size factors are calculated within
each cluster and are then scaled so they are comparable across clusters.

### Cluster cells

The table below show the number and size of clusters found:

```{r quickCluster}
set.seed(100)
clust <- quickCluster(sce, BPPARAM=bpp)
table(clust)
```

### Compute size factors

```{r calculateSumFactors}
sce <- computePooledFactors(sce,
			 clusters = clust,
			 min.mean = 0.1,
			 BPPARAM = bpp)
deconv.sf <- sizeFactors(sce)
summary(deconv.sf)
```

Note: *min.mean* - A numeric scalar specifying the minimum (library
size-adjusted) average count of genes to be used for normalization. This means
large numbers of very lowly expressed genes will not bias the normalization.

Plot deconvolution size factors against library size factors:

```{r deconv_v_librarySize}
data.frame(LibrarySizeFactors = lib.sf, 
           DeconvolutionSizeFactors = deconv.sf,
			     SampleGroup = sce$SampleGroup) %>%
  ggplot(aes(x=LibrarySizeFactors, y=DeconvolutionSizeFactors)) +
      geom_point(aes(col=SampleGroup)) +
      geom_abline(slope = 1, intercept = 0)
```

### Apply size factors

For each cell, raw counts for genes are divided by the size factor for that cell
and log-transformed so downstream analyses focus on genes with strong relative
differences. We use `scater::logNormCounts()`.

```{r logNormCounts}
sce <- logNormCounts(sce)
assayNames(sce)
```

## Save the normalised object

```{r save_normalised, eval=FALSE}
saveRDS(sce, "Robjects/caron_normalied.rds")
```

## Exercise 1

Exercise: apply the deconvolution normalisation on a single sample: ETV6-RUNX1_1
(aka GSM3872434).

You first load the same object we loaded earlier, then select cells for
SampleName 'ETV6-RUNX1_1'. You will then cluster cells, compute and apply size
factors.

# sctransform: Variant Stabilising Transformation

With scaling normalisation a correlation remains between the mean and variation
of expression (heteroskedasticity). This affects downstream dimensionality
reduction as the few main new dimensions are usually correlated with library
size. `sctransform` addresses the issue by regressing library size out of raw
counts and providing residuals to use as normalized and variance-stabilized
expression values in some downstream analyses, such as dimensionality reduction.

The `sctransform` package is from the Seurat suite of scRNAseq analysis
packages. Rather than convert our Single Cell Experiment object into a Seurat
object and use the `Seurat` package's command `SCTransform`, we will extract the
counts matrix from our SCE object and run the variance stabilising
transformation (VST) algorithm, using the `sctranform` package's `vst` command,
directly on the matrix. We can extract the counts matrix - as a *dgCMatrix*
object sparse matrix - using the `counts` function.

```{r extract_counts}
counts <- counts(sce)
class(counts)
```

## Rationale

In order to demonstrate the rationale behind the using the variance stabilising
transformation, we will visually inspect various properties of our data. Our
main interest is in the general trends not in individual outliers. Neither genes
nor cells that stand out are important at this step; we focus on the global
trends.

### Derive gene and cell attributes from the UMI matrix

#### Gene attributes

Gene attributes include for each gene:

* mean UMI count across cells
* number of cells where the gene is detected
* variance of UMI counts across cells
* the mean and variance above on the log10 scale

```{r gene_attributes}
gene_attr <- data.frame(mean = rowMeans(counts), 
                        detection_rate = rowMeans(counts > 0),
                        var = rowVars(counts)) %>% 
  mutate(log_mean = log10(mean)) %>% 
  mutate(log_var = log10(var))

rownames(gene_attr) <- rownames(counts)

dim(gene_attr)
head(gene_attr)
```

#### Cell attributes

Attributes include for each cell:

* total UMI count across genes (library size)
* number of genes detected (with at least 1 UMI)

```{r cell_attributes}
cell_attr <- data.frame(n_umi = colSums(counts),
                        n_gene = colSums(counts > 0))
rownames(cell_attr) <- colnames(counts)

dim(cell_attr)
head(cell_attr)
```

### Mean-variance relationship

For the genes, on the log10 scale we can see that up to a mean UMI count of 0
the variance follows the line through the origin with slope one, i.e. variance
and mean are roughly equal as expected under a Poisson model. However, genes
with a higher average UMI count show overdispersion compared to Poisson.

```{r overdispersion_plot, fig.align='center', fig.width=6, fig.height=6}
ggplot(gene_attr, aes(log_mean, log_var)) + 
  geom_point(alpha=0.3, shape=16) +
  geom_abline(intercept = 0, slope = 1, color='red')
```

### Mean-detection-rate relationship

In line with the previous plot, we see a lower than expected detection rate in
the medium expression range. However, for the highly expressed genes, the rate
is at or very close to 1.0 suggesting that there is no zero-inflation in the
counts for those genes and that zero-inflation is a result of overdispersion,
rather than an independent systematic bias.

```{r detection_rate_plot, fig.align='center', fig.width=6, fig.height=6}
x = seq(from = -3, to = 2, length.out = 1000)
poisson_model <- data.frame(log_mean = x,
			    detection_rate = 1 - dpois(0, lambda = 10^x))
ggplot(gene_attr, aes(log_mean, detection_rate)) + 
  geom_point(alpha=0.3, shape=16) + 
  geom_line(data=poisson_model, color='red') +
  theme_gray(base_size = 8)
```

### Cell attributes

The plot below shows the relationship between the two cell attributes computed:
library size (n_umi) and number of genes detected (n_gene).

```{r n_gene_v_n_umi_plot}
ggplot(cell_attr, aes(n_umi, n_gene)) + 
  geom_point(alpha=0.3, shape=16) + 
  geom_density_2d(size = 0.3)
```

## Method

From the
[sctransform vignette](https://htmlpreview.github.io/?https://github.com/satijalab/sctransform/blob/supp_html/supplement/variance_stabilizing_transformation.html):
"Based on the observations above, which are not unique to this particular data
set, we propose to model the expression of each gene as a negative binomial
random variable with a mean that depends on other variables. Here the other
variables can be used to model the differences in sequencing depth between
cells and are used as independent variables in a regression model. In order to 
avoid overfitting, we will first fit model parameters per gene, and then use
the relationship between gene mean and parameter values to fit parameters,
thereby combining information across genes. Given the fitted model parameters,
we transform each observed UMI count into a Pearson residual which can be
interpreted as the number of standard deviations an observed count was away
from the expected mean. If the model accurately describes the mean-variance
relationship and the dependency of mean and latent factors, then the result
should have mean zero and a stable variance across the range of expression."


In short:

* expression of a gene is modeled by a negative binomial random variable with
a mean that depends on library size
* library size is used as the independent variable in a regression model
* the model is fit for each gene, then combined data across genes is used to fit
parameters
* convert UMI counts to residuals akin to the number of standard deviations
away from the expected mean.

Assumptions:

* accurate model of the mean-variance relationship
* accurate model of the dependency of mean and latent factors

Outcome:

* the mean of the transformed data (residuals) is zero
* stable variance across expression range

## Application

### Estimation and transformation

We will now estimate model parameters and transform data.

The `vst` function estimates model parameters and performs the variance
stabilizing transformation.

Here we use the log10 of the total UMI counts of a cell as variable for
sequencing depth for each cell. After data transformation we plot the model
parameters as a function of gene mean (geometric mean). We will set the following
arguments:

* `umi` - The matrix of UMI counts with genes as rows and cells as columns  
* `latent_var` - The independent variables to regress out as a character vector  
* `return_gene_attr` - Make cell attributes part of the output  
* `return_cell_attr` - Calculate gene attributes and make part of output  

```{r sctransform_vst, eval=FALSE}
set.seed(44)
vst_out <- vst(umi = counts,
               latent_var = c('log_umi'),
               return_gene_attr = TRUE,
               return_cell_attr = TRUE
  )
```

```{r sctransform_vst_run, echo=FALSE}
set.seed(44)
vst_out <- vst(umi = counts,
               latent_var = c('log_umi'),
               return_gene_attr = TRUE,
               return_cell_attr = TRUE,
               verbosity = 0
  )
```

### Parameter plots

We will generate some diagnostic plots in order to inspect the estimated and
fitted model parameters.

By default parameters shown are:  

* intercept  
* latent variables, here log_umi  
* overdispersion factor (od_factor)  

```{r sctransform_vst_plot, eval=FALSE}
plot_model_pars(vst_out)
```

```{r sctransform_vst_plot_run, echo=FALSE, fig.width=12, fig.height=5}
plot_model_pars(vst_out, verbosity = 1)
```

We check the regression model used is the one the we intended:

```{r model_show_sct_Caron_5hCellPerSpl}
vst_out$model_str
```

We will now look at several genes in more detail by plotting observed UMI counts
and comparing these to plots using the residuals from the modelling.

For each gene of interest, we will plot:

* the observed cell attribute (UMI counts) against the latent variable (library
size) (by default), with the fitted model as a pink line showing the expected
UMI counts given the model and a shaded region spanning one standard deviation
from the expected value.
* the residuals against the latent variable in the same way.

We will look at two genes: 'RPL10' and 'HBB':

```{r goi_tab, echo=FALSE}
rowData(sce) %>%
	as.data.frame %>%
	filter(Symbol %in% c('RPL10', 'HBB')) %>%
  select(ID, Symbol, Type, Chromosome)
```

```{r vst_model_plot, fig.align='center', fig.width=8, fig.height=8}
ensId <- rowData(sce) %>%
	as.data.frame %>%
	filter(Symbol %in% c('RPL10', 'HBB')) %>%
  pull("ID")

plot_model(x = vst_out,
           umi = counts,
           goi = ensId,
           plot_residual = TRUE)
```

### Overall properties of transformed data

The distribution of residual mean is centered around 0:

```{r residual_mean_histogram}
ggplot(vst_out$gene_attr, aes(x = residual_mean)) +
	geom_histogram(binwidth=0.01)
```

The distribution of residual variance is centered around 1:

```{r residual_variance_histogram}
ggplot(vst_out$gene_attr, aes(residual_variance)) +
	geom_histogram(binwidth=0.1) +
	geom_vline(xintercept=1, color='red') +
	xlim(0, 10)
```

Plotting the residual variance against the mean shows that after transformation
there is no relationship between gene mean and variance.

```{r resVar_mean}
ggplot(vst_out$gene_attr, aes(x = log10(gmean), y = residual_variance)) +
       geom_point(alpha=0.3, shape=16)
```

Check genes with large residual variance. These genes would be markers of
expected cell populations. Note how they represent a great range of mean UMI and
detection rate values.

```{r top_genes_residual_var}
vst_out$gene_attr %>%
	top_n(n = 10) %>%
	mutate(across(where(is.numeric), round, 2)) %>% 
  rownames_to_column("ID") %>%
  left_join(as.data.frame(rowData(sce))[,c("ID", "Symbol")], "ID")
```

## Storage of the VST transformed data in the SCE object

In order to store the transformed values in our Single Cell object, we need to
add them as a new "assay". The transformed values are kept as a matrix in the
`y` object within `vst_out`.

Note that, by default, genes that are expressed in fewer than 5 cells are not
used by `vst` and results for these genes are not returned, so to add
`vst_out$y` as an assay in our single cell object we may need to subset the rows
of our `sce` object to match the rows of `vst_out$y`. In our case, none of the
genes in our data set were detected in less than 6 cells, so the `vst$y` matrix
matches our original data row for row.

```{r add_vst_to_sce}
vstMat <- as(vst_out$y[rownames(sce),], "dgCMatrix")

assay(sce, "sctrans_norm", withDimnames=FALSE) <- vstMat
```

## Exercise 2

Exercise: apply the sctransform VST normalisation on a single sample:
ETV6-RUNX1_1a (aka SRR9264343).

In exercise 1, you made a new SCE object with cells for SampleName
'ETV6-RUNX1_1'. You will now inspect the mean-variance relationship and apply
sctransform to that data.

# Session information

<!--<details>-->
```{r session_info}
sessionInfo()
```
<!--</details>-->
